---
title: "Assignment - Real-Time, Streaming Social Media "
description: |
  VAST Challenge 2021, Mini-Challenge 3
author:
  - name: Lim Jiahui, Masters of IT in Business (Analytics)
    url: https://www.linkedin.com/in/jiahui-lim-450/
    affiliation: School of Computing and Information Systems, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
date: 07-07-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      eval = TRUE,
                      fig.retina = 3)
```

> This post was written as part of requirements for Visual Analytics course in MITB. The VAST Challenge 2021 scenario and all people, places, information are all fictitious.

## 1. Introduction and Objectives

In the island country of Kronos, a Tethys-based GAStech has been operating a natural gas production site on the island, producing remarkable profits and obtained strong relations with the Kronos government officials. However, GAStech's operations has affected the environmental stewardship. In January 2014, the leaders of GAStech were celebrating their new-found fortune when suddenly, in the midst of the celebration, several employees disappear. An organization known as Protectors of Kronos (POK) were suspected to be involved in the disappearance.

On the day of January 23 2014, there are also multiple events happening across Abila, a town in Kronos. Social media of the public as well as communication transcripts of the emergency departments in Abila were reocrded for analysis. As a analyst for this case, the aim of the challenge is to utilize text and visual analytics and evaluate the change of risk levels as well as recommend further actions for better emergency responses.


## 2. Literature Review

### 2.1 Past Mini-Challenge 3 Approaches

Past Mini-Challenge methods to deal with spam --> spam index




### 2.2 R Packages

The table below shows the R packages that will be used in this challenge.

| Package | Description |
|:-------|:-------------|
| tidyverse |   |
| dplyr |  |
| ggplot2 |   |
| plotly |   |






## 3. Exploratory Data Analysis and Data Wrangling

### 3.1 Setting up environment

The data given in this challenge consists of three csv files, each spanning across different time period (1700-1830, 1831-2000, 2001-2131). In each file, the data includes mbdata (microblog) and ccdata (call centre), which then further elaborated with details such as datetime, message and location. Geospatial and Aspatial files were also included for location tracking.


The code chunk below dictates the installation or loading of packages.

```{r echo=TRUE}
packages = c('lubridate','tidyverse', 'ggplot2', 'ggraph', 'igraph', 'lsa', 'DT',
             'tm', 'wordcloud', 'tidytext', 'dplyr','textmineR', 'ggExtra', 'stringr',
             'TSstudio','hrbrthemes')

for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```


### 3.2 Data Extraction and Preparation

Uploading csv data and combining all together

```{r}
csv1 <- read_csv('data/csv-1700-1830.csv')
csv2 <- read_csv('data/csv-1831-2000.csv')
csv3 <- read_csv('data/csv-2001-2131.csv')

# sample glimpse of one csv as the rest are similar
#glimpse(csv1)

data <- rbind(csv1, csv2, csv3)
glimpse(data)

```


Converting time format and extracting the time

```{r}
data$datetime <- ymd_hms(data$`date(yyyyMMddHHmmss)`)
data$time <- format(as.POSIXct(data$datetime), format = "%H:%M:%S")
glimpse(data$datetime)

```


checking NAs

```{r}
indx <- apply(data, 2, function(x) any(is.na(x) | is.infinite(x)))
indx

```



### 3.3 Pre-processing Analysis

Preparing data by seperating call centre data and microblog data. 

Create time bins

```{r}
# separating cc and mb data
cc <- data %>% filter(type=="ccdata")
mb <- data %>% filter(type=="mbdata")

# creating time bin for analysis
cc$timebin <- "-"
cc[cc$time < "17:30:00", "timebin"] <- "5-5:30pm"
cc[cc$time >= "17:30:00" & cc$time < "18:00:00", "timebin"] <- "5:30pm-6pm"
cc[cc$time >= "18:00:00" & cc$time < "18:30:00", "timebin"] <- "6-6:30pm"
cc[cc$time >= "18:30:00" & cc$time < "19:00:00", "timebin"] <- "6:30pm-7pm"
cc[cc$time >= "19:00:00" & cc$time < "19:30:00", "timebin"] <- "7-7:30pm"
cc[cc$time >= "19:30:00" & cc$time < "20:00:00", "timebin"] <- "7:30pm-8pm"
cc[cc$time >= "20:00:00" & cc$time < "20:30:00", "timebin"] <- "8-8:30pm"
cc[cc$time >= "20:30:00" & cc$time < "21:00:00", "timebin"] <- "8:30pm-9pm"
cc[cc$time >= "21:00:00", "timebin"] <- "9-9:30pm"

```

Plotting call frequency received by call center wrt to time bins

```{r echo=FALSE}
# getting call frequency from 1700-2131
freq <- cc %>% group_by(timebin) %>%
  summarize(num = n())

#par(mar=c(5,6,4,1)+.1)

# Uniform color
bar1 <- barplot(height=freq$num, names=freq$timebin, col=c(rgb(0.3,0.1,0.4,0.6) , rgb(0.3,0.5,0.4,0.6)),
                main="Call Frequencies per Time Interval", ylim=c(0,50))
# Add the text 
text(bar1, freq$num, paste("Freq: ", freq$num, sep=""), cex=1, pos=3)

```



Types of call received by call center

```{r fig.align="center", echo = FALSE, fig.width=10, fig.height=10}
# type of calls
types <- unique(cc$message)
count_type <- cc %>% group_by(message, timebin) %>% 
  summarize(count = n()) 

# setting margin
#par(mar = c(5, 5, 5, 5)) 

ggplot(count_type, aes(x = count, y = message, fill = timebin,
                       stringr::str_wrap(count_type$message, 15))) +
  geom_bar(position="stack",stat="identity") +
  scale_fill_viridis(discrete = T,option = "H") +
  theme_ipsum() +
  ggtitle("Calls per Message and Time Interval") +
  theme(axis.text.x = element_text(face="bold", color="#000092",
                                   size=8, angle=0),
        axis.text.y = element_text(face="bold", color="#000092",
                                   size=8, angle=0))

```


Number of Microblogs per time interval (interactive)

```{r echo = FALSE}
mb_per_time <- mb %>% group_by(as.POSIXct(datetime, format = "%H:%M:%S")) %>% summarize(no_microblogs = n())

ts_plot(mb_per_time,
        title = "Microblog Frequency Per Time",
        Xtitle = "Time",
        Ytitle = "Frequency",
        line.mode = "lines+markers",
        slider = TRUE)

```



### 3.4 Text Pre-processing and Exploratory Analysis

Using text mining package to create corpus, lowercase, remove punctuations, remove stopwords and stemming


#### Considering all words

```{r echo = FALSE}
# Build a corpus containing all the microblogs
corpus <- VCorpus(VectorSource(mb$message))

# convert the text to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
corpus<-tm_map(corpus, PlainTextDocument)

# for debugging
#writeLines(as.character(corpus[[5]])) # inspect doc number 5

# creating own punctuation removal instead of using from tm as they do not insert blank spaces
replacePunctuation <- content_transformer(function(x) {return (gsub("[[:punct:]]"," ", x))})

# remove all punctuation from the corpus
corpus<-tm_map(corpus, replacePunctuation)

# remove all numbers
corpus<-tm_map(corpus, removeNumbers)

# remove all English stopwords from the corpus
corpus<-tm_map(corpus, removeWords, stopwords("en"))

# stemming the words to its root form in the corpus
corpus<-tm_map(corpus, stemDocument)

# creating document term matrix from corpus
Doc_mat <- DocumentTermMatrix(corpus)

# term matrix and get the frequency counts of each word
term_doc_mat <- TermDocumentMatrix(corpus)
tokens <- as.matrix(term_doc_mat)
f <- sort(rowSums(tokens),decreasing=TRUE)
d <- data.frame(word = names(f),freq=f)

# showing top 10 words
#head(d, 10)
#barplot(f, las=2, col = rainbow(25))

#showing top 200 words via wordclous
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```


#### Showing Hashtags only

```{r echo = FALSE}

mb$hashtags <- "-"

for (i in 1:nrow(mb)){
  mb$hashtags[i] <- str_extract_all(mb$message[i], "#\\S+")
}

# Build a corpus containing all hashtags
HT.corpus <- VCorpus(VectorSource(mb$hashtags))

# convert the text to lowercase
HT.corpus <- tm_map(HT.corpus, content_transformer(tolower))
HT.corpus<-tm_map(HT.corpus, PlainTextDocument)

# term matrix and get the frequency counts of each hashtag
HT.tdm <- TermDocumentMatrix(HT.corpus)
HT <- as.matrix(HT.tdm)
HT.f <- sort(rowSums(HT),decreasing=TRUE)
HT.d <- data.frame(word = names(HT.f),freq=HT.f)

# showing top 10 words
#head(HT.d, 10)


set.seed(1234)
wordcloud(words = HT.d$word, freq = HT.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

#### Top Mentions

```{r echo = FALSE}
mb$mentions <- "-"

for (i in 1:nrow(mb)){
  mb$mentions[i] <- str_extract_all(mb$message[i], "@\\S+")
}

# Build a corpus containing all hashtags
MEN.corpus <- VCorpus(VectorSource(mb$mentions))

# convert the text to lowercase
MEN.corpus <- tm_map(MEN.corpus, content_transformer(tolower))
MEN.corpus<-tm_map(MEN.corpus, PlainTextDocument)

# term matrix and get the frequency counts of each hashtag
MEN.tdm <- TermDocumentMatrix(MEN.corpus)
MEN <- as.matrix(MEN.tdm)
MEN.f <- sort(rowSums(MEN),decreasing=TRUE)
MEN.d <- data.frame(word = names(MEN.f),freq=MEN.f)

# showing top 10 words
#head(HT.d, 10)


set.seed(1234)
wordcloud(words = MEN.d$word, freq = MEN.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


```





#### Showing words without hashtags and mentions

Removing all hashtags, mentions and retweets

```{r echo = FALSE}
mb$words.only <- "-"

# remove retweets too (akaa "RT @____")
for (i in 1:nrow(mb)){
  mb$words.only[i] <- sub("(?:\\s*#\\w+)+\\s*$", " ", mb$message[i]) #hashtags
  mb$words.only[i] <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", mb$words.only[i])  #retweets
  mb$words.only[i] <- gsub("@\\S+", " ", mb$words.only[i])   #normal mention
}

# Build a corpus containing all the words
corpus_words <- VCorpus(VectorSource(mb$words.only))

# convert the text to lowercase
corpus_words <- tm_map(corpus_words, content_transformer(tolower))
corpus_words<-tm_map(corpus_words, PlainTextDocument)

# creating own punctuation removal instead of using from tm as they do not insert blank spaces
replacePunctuation <- content_transformer(function(x) {return (gsub("[[:punct:]]"," ", x))})

# remove all punctuation from the corpus
corpus_words<-tm_map(corpus_words, replacePunctuation)

corpus_words<-tm_map(corpus_words, removeNumbers)

# remove all English stopwords from the corpus
corpus_words<-tm_map(corpus_words, removeWords, stopwords("en"))

# stemming the words to its root form in the corpus
corpus_words<-tm_map(corpus_words, stemDocument)

# term matrix and get the frequency counts of each word
tdm_words <- TermDocumentMatrix(corpus_words)
w <- as.matrix(tdm_words)
w.f <- sort(rowSums(w),decreasing=TRUE)
w.d <- data.frame(word = names(w.f),freq=w.f)

# showing top 10 words
#head(w.d, 10)

# wordcloud
set.seed(1234)
wordcloud(words = w.d$word, freq = w.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


```







## 4. Unsupervised Text Classification (Q1)

>Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.


Distinguishing event reports from junk or spam using tf-idf (higher tf-idf = rarer term)


To prevent unnecessary noises, we will use texts that are preprocessed and without hashtags, retweets or mentions.


USING TIDY TEXT
 
```{r echo = FALSE}
text_df <- tibble(line = 1:nrow(mb), text = mb$words.only)

# tokenizations and removal of stopwords, numbers and patterns
text_df <- text_df %>% 
  mutate(text_df, text = gsub(x = text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", 
                              replacement = " ")) %>% unnest_tokens(word,text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

#count_textdf <- text_df %>% count(word, sort = TRUE)

## plotting word counts (those greater than the mean word count)
# set.seed(1234)
# count_textdf %>%
#   filter(n > mean(count_textdf$n)) %>%
#         graph_from_data_frame() %>%
#         ggraph(layout = "fr") +
#         # geom_edge_link(aes(edge_alpha = n, edge_width = n))
#         # geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
#         geom_node_point(color = "darkslategray4", size = 3) +
#         geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
#         labs(title = "Word Cluster: Words Frequently Occur",
#              subtitle = "Microblog Messages ",
#              x = "", y = "")


text_df <- text_df %>%
  group_by(line) %>%
  count(word, sort = TRUE) %>%
  ungroup()

tfidf <- text_df %>%
  bind_tf_idf(word, line, n) %>%
  arrange(desc(tf_idf))

# Interactive table for TF-IDF
DT::datatable(tfidf, filter = 'top') %>%
  formatRound(columns = c('tf','idf','tf_idf'),
              digits = 3) %>%
  formatStyle(0,
              target = 'row',
              lineHeight = '25%')

```
Getting spread of TF-IDF

```{r echo = FALSE}
# #giving word id
# words.tfidf$word_id <- "-"
# for (i in 1:nrow(words.tfidf)){
#   words.tfidf$word_id[i] <- i
# }

tfidf %>%
  ggplot( aes(y=tf_idf)) +
    geom_boxplot() +
    # scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    # geom_jitter(color="black", size=0.4, alpha=0.9) +
    # theme_ipsum() +
    # theme(
    #   legend.position="none",
    #   plot.title = element_text(size=11)
    # ) +
    ggtitle("A boxplot for TF-IDF") +
    xlab("")

```

Based on boxplot, filter threshold of TF-IDF > = 6

```{r echo = FALSE}
tfidf.cluster <- tfidf[, c('word','tf_idf')]

set.seed(1234)
tfidf.cluster %>%
  filter(tf_idf >= 6) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        # geom_edge_link(aes(edge_alpha = n, edge_width = n))
        # geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "TF-IDF Cluster: Junk Detection",
             subtitle = "Microblog Messages ",
             x = "", y = "")

```

Getting sentences of words with tf-idf >= 6

```{r echo = FALSE}
junk <- tfidf %>% filter(tf_idf >= 6)

junk$message <- "-"
for (i in 1:nrow(junk)){
  junk$message[i] <- mb$message[junk$line[i]]
}

DT::datatable(junk, filter = 'top', options = list(autoWidth = FALSE)) %>%
  formatRound(columns = c('tf','idf','tf_idf'),
              digits = 3) %>%
  formatStyle(0,
              target = 'row')

```






Detecting spam index by author - lots of messages but low spam index = spam

```{r echo=FALSE}
author_spam <- mb %>% 
  group_by(author) %>% 
  summarize(no_messages = n(), distinct_msg = n_distinct(message)) %>% 
  mutate(spam_index = distinct_msg/no_messages) %>%
  arrange(spam_index)


# ggplot(author_spam)  + 
#     geom_bar(aes(x=author, y=no_messages),stat="identity", fill="tan1", colour="sienna3") +
#     geom_point(aes(x=author, y=spam_index), size=1, color="black") +
#     # geom_text(aes(label=spam_index, x=author, y=spam_index), colour="black")+
#     # geom_text(aes(label=no_messages, x=author, y=no_messages), colour="black")+
#     scale_y_continuous(sec.axis = sec_axis(~.*0.001, name="spam_index"))
# 


# giving authors a number
author_spam$id <- 1:nrow(author_spam)

#showing table
DT::datatable(author_spam, filter = 'top', 
              options = list(autoWidth = FALSE)) %>%
  formatStyle(0,
              target = 'row')




## add extra space to right margin of plot within frame
par(mar=c(5, 6, 4, 6) + 0.2)

## Plot first set of data and draw its axis
plot(author_spam$id, author_spam$no_messages, pch=16, 
     axes=FALSE, ylim=c(0,1500), xlab="", ylab="", 
     type="b",col="black", main="Spam Detection by Author")
axis(2, ylim=c(0,1),col="black",las=1)  ## las=1 makes horizontal labels
mtext("No. of messages",side=2,line=4)
box()

## Allow a second plot on the same graph
par(new=TRUE)

## Plot the second plot and put axis scale on right
plot(author_spam$id, author_spam$spam_index, 
     pch=15,  xlab="", ylab="", ylim=c(0,1), 
     axes=FALSE, type="b", col="red")

## a little farther out (line=4) to make room for labels
mtext("Spam Index",side=4,col="red",line=3) 
axis(4, ylim=c(0,7000), col="red",col.axis="red",las=1)

## Draw the time axis
axis(1,pretty(range(author_spam$id),10))
mtext("Author's ID",side=1,col="black",line=2.5)  

## Add Legend
# legend("topright",legend=c("Beta Gal","Cell Density"),
#   text.col=c("black","red"),pch=c(16,15),col=c("black","red"))

```























## 5. Topic Modeling and Sentiment Analysis (Q1, Q2)

> Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.



Time-series Analysis
-showing how top 10 words changes across time????


To prevent unnecessary noises, we will use texts that are preprocessed and without hashtags, retweets or mentions.



#### Topic Modeling by Cosine Similarity and Walktrap Algorithm

mapping words with cosine similarity (finding how similar words are irrespective of their size) > 80th percentile

Cosine Similarity close to 1 = vectors are similar

- Word similarity are calculated based on how often certain group of words co-occur.
- Using Walktrap algorithm to auto-generated number of topics and cluster nodes together

```{r}
cosine_matrix <- function(tokenized_data, lower = 0, upper = 1, filt = 0) {
  
  if (!all(c("word", "line") %in% names(tokenized_data))) {
    stop("tokenized_data must contain variables named word and id")
  }
  
  if (lower < 0 | lower > 1 | upper < 0 | upper > 1 | filt < 0 | filt > 1) {
    stop("lower, upper, and filt must be 0 <= x <= 1")
  } 
  
  docs <- length(unique(tokenized_data$line))
  
  out <- tokenized_data %>%
    count(line, word) %>%
    group_by(word) %>%
    mutate(n_docs = n()) %>%
    ungroup() %>%
    filter(n_docs < (docs * upper) & n_docs > (docs * lower)) %>%
    select(-n_docs) %>%
    mutate(n = 1) %>%
    spread(word, n, fill = 0) %>%
    select(-line) %>%
    as.matrix() %>%
    lsa::cosine()
  
  filt <- quantile(out[lower.tri(out)], filt)
  out[out < filt] <- diag(out) <- 0
  out <- out[rowSums(out) != 0, colSums(out) != 0]
  
  return(out)
}


cos_mat <- cosine_matrix(text_df, lower = .01, upper = .80, filt = .80)


# graph
g <- graph_from_adjacency_matrix(cos_mat, mode = "undirected", weighted = TRUE)

set.seed(1234)
ggraph(g, layout = "nicely") +
  geom_edge_link(aes(alpha = weight, width = weight), show.legend = FALSE) + 
  geom_node_label(aes(label = name)) +
  theme_void()

```


```{r}
walktrap_topics <- function(g, ...) {
  wt <- igraph::cluster_walktrap(g, ...)
  
  membership <- igraph::cluster_walktrap(g, ...) %>% 
      igraph::membership() %>% 
      as.matrix() %>% 
      as.data.frame() %>% 
      rownames_to_column("word") %>% 
      arrange(V1) %>% 
      rename(group = V1)
  
  dendrogram <- stats::as.dendrogram(wt)
  
  return(list(membership = membership, dendrogram = dendrogram))
}


topics <- walktrap_topics(g)

topics$membership %>% 
  group_by(group) %>% 
  summarise(words = paste(word, collapse = ", "))

```


```{r}

V(g)$cluster <- arrange(topics$membership, word)$group

set.seed(1234)
ggraph(g, layout = "nicely") +
  geom_edge_link(aes(alpha = weight, width = weight), show.legend = FALSE) + 
  geom_node_label(
    aes(label = name, color = factor(cluster)), 
    show.legend = FALSE
  ) +
  theme_void()

```



Top Events = from the topics



### Time-series Analysis of Topics

Using time-bins?

TF-IDF vs Time

```{r}
# groupings <- topics$membership %>% 
#   group_by(group) %>% 
#   summarise(words = paste(word, collapse = ", "))

topic_words <- unique(topics$membership$word)
groupings <- topics$membership

topics_df <- data.frame(matrix(ncol=0, nrow=nrow(text_df)))
topics_df$line <- "-"
topics_df$word <- "-"
topics_df$message <- "-"
topics_df$time <- "-"
topics_df$group <- "-"


for (i in 1:nrow(text_df)){
  if (text_df$word[i] %in% topic_words){
    no <- text_df$line[i]
    topics_df$line[i] <- no
    topics_df$message[i] <- mb$message[no]
    topics_df$time[i] <- mb$time[no]
    topics_df$group[i] <- groupings$group[which(groupings$word==(text_df$word[i]))]
    topics_df$word[i] <- text_df$word[i]
  }
}


#distinct(topics_df)


#topics_df %>% filter_at(!"-")

```










### Sentiment Analysis








## 6. Geospatial Analysis with mbdata and ccdata (Q3)

> If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words.


Mapping event vs time + calls received vs time



## 7. Conclusion and Recommendations




---
#### References

- [VAST Challenge 2021](https://vast-challenge.github.io/2021/MC3.html)
- [VAST Challenge 2014](http://www.vacommunity.org/VAST+Challenge+2014)
- [VAST Challenge Mini-Challenge 3, UBA-Rukina-Group51-MC3](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC3%20-%20Real-Time,%20Streaming%20Social%20Media/entries/University%20of%20Buenos%20Aires%20-%20Rukavina/)
- [Plotting Graphs in R](https://www.r-graph-gallery.com/index.html)
- [Tidytextmining](https://www.tidytextmining.com/topicmodeling.html)
- [Clustering Topics](https://www.markhw.com/blog/word-similarity-graphs)
- [Time-series Plotting](https://cran.r-project.org/web/packages/TSstudio/vignettes/Plotting_Time_Series.html)
- [Word Similarity - using Walktrap Algorithm](https://www.markhw.com/blog/word-similarity-graphs)






















